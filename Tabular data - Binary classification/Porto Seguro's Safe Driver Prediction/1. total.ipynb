{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Binary classification\n","\n","- Porto Seguro's Safe Driver Prediction\n","- [자료1](https://www.kaggle.com/bertcarremans/data-preparation-exploration), [자료2](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial), [자료3](https://www.kaggle.com/aharless/xgboost-cv-lb-284), [자료4](https://www.kaggle.com/gpreda/porto-seguro-exploratory-analysis-and-prediction)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# 기본: 데이터 다루기\n","import numpy as np\n","import pandas as pd\n","from PIL import Image, ImageDraw, ImageFont\n","# 정규식\n","import re\n","import time\n","\n","# Numba: python 연산을 더 빠르게 해주는 compiler를 이용할 수 있게 하는 library\n","from numba import jit\n","# compile 후 이용하듯 먼저 작은 값으로 연산시켜서 구성을 저장하고 실제 큰 값을 최적화시켜 이용하는 방식\n","import gc # Garbage Collector\n","\n","from collections import Counter\n","import missingno as msno\n","\n","# Sklearn package, 모델 적합에 이용\n","# sklearn.preprocessing.Imputer는 0.22 ver에서 삭제되었다.\n","from sklearn.utils import shuffle\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder\n","\n","from sklearn.feature_selection import VarianceThreshold, SelectFromModel, mutual_info_classif\n","from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","\n","# plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from IPython.display import Image as PImage\n","\n","# 새로운 plotting package 등장\n","import plotly.offline as py\n","import plotly.graph_objs as go\n","import plotly.tools as tls\n","py.init_notebook_mode(connected=True)\n","\n","# option\n","pd.set_option(\"display.max_columns\", 100)\n","from subprocess import check_call\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"../input/porto-seguros-safe-driver-prediction-dataset/train.csv\")\n","test = pd.read_csv(\"../input/porto-seguros-safe-driver-prediction-dataset/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.drop_duplicates()\n","train.shape"]},{"cell_type":"markdown","metadata":{},"source":["내 이럴 줄 알았다. inplace 매개변수를 쓰던, output을 원 데이터로 해주던 초기화해주는 장치가 있어야 drop_duplicates()한 것을 저장할 수 있다."]},{"cell_type":"markdown","metadata":{},"source":["## preprocessing: Metadata\n","\n","- feature를 이용한 변수 분석, 시각화, 모델링 등에 도움이 되어 (좀 더 실정에 맞는 모델을 구성할 수 있음) 데이터 DataFrame 자체의 metadata를 아는 것은 중요하다.\n","- 그런데 여기서 얘기하는 metadata는 feature 데이터의 자체적 특성(or 분류)를 말하는 것 같다."]},{"cell_type":"markdown","metadata":{},"source":["- **role**: input, ID, target\n","- **level**: nominal, interval, ordinal, binary\n","- **keep**: True or False\n","- **dtype**: int, float, str"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","for f in train.columns:\n","    if f == \"target\":\n","        role = \"target\"\n","    elif f == \"id\":\n","        role = \"id\"\n","    else:\n","        role = \"input\"\n","    \n","    if \"bin\" in f or f == \"target\":\n","        level = \"binary\"\n","    elif \"cat\" in f or f == \"id\":\n","        level = \"nominal\"\n","    elif train[f].dtype == float:\n","        level = \"interval\"\n","    else:\n","        level = \"ordinal\"\n","    \n","    keep = True\n","    if f == \"id\":\n","        keep = False\n","    \n","    category = \"none\"\n","    if \"ind\" in f:\n","        category = \"individual\"\n","    elif \"reg\" in f:\n","        category = \"registration\"\n","    elif \"car\" in f:\n","        category = \"car\"\n","    elif \"calc\" in f:\n","        category = \"calculated\"\n","    \n","    dtype = train[f].dtype\n","    f_dict = {\"varname\": f, \"role\": role, \"level\": level, \"keep\": keep, \"dtype\": dtype,\n","              \"category\": category}\n","    data.append(f_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["meta = pd.DataFrame(data, columns=[\"varname\", \"role\", \"level\", \"keep\", \"dtype\", \"category\"])\n","meta.set_index(\"varname\", inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["meta"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["meta[(meta.level == \"nominal\") & (meta.keep)].index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pd.DataFrame({\"count\": meta.groupby([\"category\"])[\"category\"].size()}).reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pd.DataFrame({\"count\": meta.groupby([\"role\", \"level\"])[\"role\"].size()}).reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["## Descriptive statistics\n","\n","- explore the categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"interval\") & (meta.keep)].index\n","train[v].describe()"]},{"cell_type":"markdown","metadata":{},"source":["reg variables\n","\n","- -1 = NA이기 때문에 reg_03에서 결측값 존재 (설명에선 하나래)\n","- 다른 reg에 비해 (reg_03의) max 값이 꽤 큰 편이라 표준화시킬 예정인가봄\n","\n","car variables\n","\n","- car_12, car_14에 NA 존재\n","- 여기도 13, 15의 max가 큰 편이라 표준화시킬 생각인 듯하다.\n","\n","calc variables\n","\n","- not missing values\n","- 굳이 표준화시킬 필요 없음."]},{"cell_type":"markdown","metadata":{},"source":["### Ordinal variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\n","train[v].describe()"]},{"cell_type":"markdown","metadata":{},"source":["car_11에만 missing values"]},{"cell_type":"markdown","metadata":{},"source":["### Binary variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"binary\") & (meta.keep)].index\n","train[v].describe()"]},{"cell_type":"markdown","metadata":{},"source":["not missing values, and don't use scaler"]},{"cell_type":"markdown","metadata":{},"source":["## Handling imbalanced classes"]},{"cell_type":"markdown","metadata":{},"source":["#### 작업하기 전에 target data 분포에 대해 확인 먼저 하기"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = [go.Bar(x=train[\"target\"].value_counts().index.values,\n","               y=train[\"target\"].value_counts().values,\n","               text=\"Distribution of target variable\")]\n","\n","layout = go.Layout(title=\"Target variable distribution\")\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename=\"basic-bar\")"]},{"cell_type":"markdown","metadata":{},"source":["이를 통해 target data가 불균형하게 분포하고 있음을 알 수 있다. 불균형을 최소화하는 방향으로."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Counter(train.dtypes.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["desired_apriori = 0.10\n","\n","idx_0 = train[train.target == 0].index\n","idx_1 = train[train.target == 1].index\n","\n","nb_0 = len(train.loc[idx_0])\n","nb_1 = len(train.loc[idx_1])\n","\n","undersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\n","undersampled_nb_0 = int(undersampling_rate * nb_0)\n","print(\"Rate to undersample records with target=0: {}\".format(undersampling_rate),\n","      \"Number of records with target=0 after undersampling: {}\".format(undersampled_nb_0),\n","      sep=\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n","idx_list = list(undersampled_idx) + list(idx_1)\n","train = train.loc[idx_list].reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Quality Checks\n","\n","결측값 찾기"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.isnull().any().any()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vars_with_missing = []\n","\n","for f in train.columns:\n","    missings = train[train[f] == -1][f].count()\n","    if missings > 0:\n","        vars_with_missing.append(f)\n","        missings_perc = missings / train.shape[0]\n","        \n","        print(\"Variables {} has {} records ({:.2%}) with missing values\"\n","             .format(f, missings, missings_perc))\n","\n","print(\"In total, there are {} variables with missing values\".format(len(vars_with_missing)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_copy = train\n","train_copy = train_copy.replace(-1, np.NaN)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["msno.matrix(train_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"]},{"cell_type":"markdown","metadata":{},"source":["생각보다 결측값이 많네. car_02랑 car_11만 없는 것으로 보인다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vars_to_drop = [\"ps_car_03_cat\", \"ps_car_05_cat\"]\n","train.drop(vars_to_drop, inplace=True, axis=1)\n","meta.loc[(vars_to_drop), \"keep\"] = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_imp = SimpleImputer(missing_values=-1, strategy=\"mean\")\n","mode_imp = SimpleImputer(missing_values=-1, strategy=\"mose_frequent\")\n","train[\"ps_reg_03\"] = mean_imp.fit_transform(train[[\"ps_reg_03\"]]).ravel()\n","train[\"ps_car_12\"] = mean_imp.fit_transform(train[[\"ps_car_12\"]]).ravel()\n","train[\"ps_car_14\"] = mean_imp.fit_transform(train[[\"ps_car_14\"]]).ravel()\n","train[\"ps_car_11\"] = mean_imp.fit_transform(train[[\"ps_car_11\"]]).ravel()"]},{"cell_type":"markdown","metadata":{},"source":["### Checking the cardinality of the categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"nominal\") & (meta.keep)].index\n","for f in v:\n","    dist_values = train[f].value_counts().shape[0]\n","    print(\"Variable {} has {} distinct values\".format(f, dist_values))"]},{"cell_type":"markdown","metadata":{},"source":["Script by https://www.kaggle.com/ogrellier<br />\n","Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def add_noise(series, noise_level):\n","    return series * (1 + noise_level * np.random.randn(len(series)))"]},{"cell_type":"markdown","metadata":{},"source":["assert: 가정 설정문\n","\n","- 형식: assert 조건, 메시지\n","- 조건을 만족하지 않으면 AssertError를 준다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def target_encode(trn_series=None, tst_series=None, val_series=None, target=None,\n","                  min_samples_leaf=1, smoothing=1, noise_level=0):\n","    assert len(trn_series) == len(target)\n","    assert trn_series.name == tst_series.name\n","    \n","    temp = pd.concat([trn_series, target], axis=1)\n","    averages = temp.groupby(trn_series.name)[target.name].agg([\"mean\", \"count\"])\n","    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n","    prior = target.mean()\n","    \n","    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n","    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n","    \n","    ft_trn_series = pd.merge(\n","        trn_series.to_frame(trn_series.name),\n","        averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n","        on=trn_series.name, how=\"left\"\n","    )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n","    ft_trn_series.index = trn_series.index\n","    \n","    \n","    ft_tst_series = pd.merge(\n","        tst_series.to_frame(tst_series.name),\n","        averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n","        on=tst_series.name, how=\"left\"\n","    )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n","    ft_tst_series.index = tst_series.index\n","    \n","    if val_series is not None:\n","        ft_val_series = pd.merge(\n","            val_series.to_frame(val_series.name),\n","            averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n","            on=val_series.name, how=\"left\"\n","        )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n","        ft_val_series.index = val_series.index\n","        \n","        return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n","\n","    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], test[\"ps_car_11_cat\"],\n","                                            target=train.target, min_samples_leaf=100,\n","                                            smoothing=10, noise_level=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train[\"ps_car_11_cat_te\"] = train_encoded\n","train.drop(\"ps_car_11_cat\", axis=1, inplace=True)\n","\n","meta.loc[\"ps_car_11_cat\", \"keep\"] = False\n","\n","test[\"ps_car_11_cat_te\"] = test_encoded\n","test.drop(\"ps_car_11_cat\", axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Visualization"]},{"cell_type":"markdown","metadata":{},"source":["### Categorical variables\n","\n","- \\_, ax = plt.subplots(3, 4)로 구제하고 싶었지만, sns라 그럴 수 없었다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"nominal\") & (meta.keep)].index\n","for f in v:\n","    plt.figure(figsize=(20, 10))\n","    \n","    cat_perc = train[[f, \"target\"]].groupby([f], as_index=False).mean()\n","    cat_perc.sort_values(\"target\", ascending=False, inplace=True)\n","    \n","    sns.barplot(f, \"target\", data=cat_perc, order=cat_perc[f])\n","    \n","    plt.ylabel(\"% target\", fontsize=18)\n","    plt.xlabel(f, fontsize=18)\n","    plt.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["데이터 타입만 고려한 거라 다를 수 있지만, interval이 float를 대변하기에 int에 관한 것도 찍어봤다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ordinal이 int가 맞을까\n","v = meta[(meta.dtype == int) & (meta.keep)].index\n","plotting_data = [go.Heatmap(\n","    z=train[v].corr().values, x=train[v].columns.values, y=train[v].columns.values,\n","    colorscale=\"Viridis\", reversescale=False, opacity=1.0)] # text=True\n","\n","layout = go.Layout(\n","    title=\"Pearson Correlation of Integer-type features\",\n","    xaxis=dict(ticks='', nticks=36), yaxis=dict(ticks=''), width=900, height=700)\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename=\"labelled-heatmap\")"]},{"cell_type":"markdown","metadata":{},"source":["### Interval variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["intervals = meta[(meta.level == \"interval\") & (meta.keep)].index\n","targets = meta[(meta.role == \"target\")].index\n","\n","mf = mutual_info_classif(train[intervals].values, train[targets].values, n_neighbors=3,\n","                         random_state=17)\n","print(mf)"]},{"cell_type":"markdown","metadata":{},"source":["KNN distance(entropy)를 기반으로 interval type input과 target 간의 밀접도 또는 의존도(결과적으로 상관계수)를 확인했다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def corr_heatmap(v):\n","    correlations = train[v].corr()\n","    \n","    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","#     cmap = plt.cm.magma\n","    plt.figure(figsize=(10, 10))\n","    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt=\".2f\",\n","                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"interval\") & (meta.keep)].index\n","corr_heatmap(v)"]},{"cell_type":"markdown","metadata":{},"source":["고려해야 할 상관계수\n","\n","- reg_02, reg_03: 0.7\n","- car_12, car_13: 0.67\n","- car_12, car_14: 0.58\n","- car_13, car_15: 0.53"]},{"cell_type":"markdown","metadata":{},"source":["컴퓨터 부담 덜 주겠다고 sampling하시겠답디다. 그리고 sampling했기 때문에 그림이 다를 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["s = train.sample(frac=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(15, 15))\n","sns.lmplot(\"ps_reg_02\", \"ps_reg_03\", data=s, hue=\"target\", palette=\"Set1\",\n","           scatter_kws={\"alpha\": 0.3})\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(15, 15))\n","sns.lmplot(\"ps_car_12\", \"ps_car_13\", data=s, hue=\"target\", palette=\"Set1\",\n","           scatter_kws={\"alpha\": 0.3})\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(15, 15))\n","sns.lmplot(\"ps_car_12\", \"ps_car_14\", data=s, hue=\"target\", palette=\"Set1\",\n","           scatter_kws={\"alpha\": 0.3})\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(15, 15))\n","sns.lmplot(\"ps_car_13\", \"ps_car_15\", data=s, hue=\"target\", palette=\"Set1\",\n","           scatter_kws={\"alpha\": 0.3})\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["심심하니까 ps_car_12랑 ps_car_15도 확인해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample = s[[\"ps_car_12\", \"ps_car_15\", \"target\"]]\n","sns.pairplot(sample, hue=\"target\", palette=\"Set1\", diag_kind=\"kde\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["저것만 보기엔 그래프가 굉장히 많으 정보를 담고 있으니 상관계수가 높은 feature는 다 봐보자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["var = [\"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\", \"ps_car_15\", \"target\"]\n","sample = s[var]\n","sns.pairplot(sample, hue=\"target\", palette=\"Set1\", diag_kind=\"kde\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["target에 대해 가우스 분포 쪽으로 한 번 더 확인해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"interval\") & (meta.keep)].index\n","t1, t0 = train.loc[train[\"target\"] != 0], train.loc[train[\"target\"] == 0]\n","\n","# sns.set_style(\"whitegrid\")\n","plt.figure()\n","_, _ = plt.subplots(3, 4, figsize=(16, 12))\n","\n","i = 0\n","for feature in v:\n","    i += 1\n","    plt.subplot(3, 4, i)\n","    \n","    sns.kdeplot(t1[feature], bw=.5, label=\"target = 1\")\n","    sns.kdeplot(t0[feature], bw=.5, label=\"target = 0\")\n","    \n","    plt.ylabel(\"Density plot\", fontsize=12)\n","    plt.xlabel(feature, fontsize=12)\n","    locs, labels = plt.xticks()\n","    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Checking the Binary features inspection\n","\n","0, 1 비율을 모든 binary에 대해 한 번에 나타내고자 함."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bin_col = meta[(meta.level == \"binary\") & (meta.keep)].index\n","bin_col = train[bin_col].columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["zero_list = []\n","one_list = []\n","for col in bin_col:\n","    zero_list.append((train[col] == 0).sum())\n","    one_list.append((train[col] == 1).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trace1 = go.Bar(x=bin_col, y=zero_list, name=\"Zero count\")\n","trace2 = go.Bar(x=bin_col, y=one_list, name=\"One count\")\n","\n","layout = go.Layout(barmode=\"stack\", title=\"Count of 1 and 0 in binary variables\")\n","fig = go.Figure(data=[trace1, trace2], layout=layout)\n","py.iplot(fig, filename=\"stacked-bar\")"]},{"cell_type":"markdown","metadata":{},"source":["interval이 그랬던 것처럼 얘도 가우스분포(앞에도 그랬지만, 다차항이 아니고 정확히 정규분포도 아니라서 그냥 종모양 분포를 얘기하는 것)로 확인해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sns.set_style(\"whitegrid\")\n","plt.figure()\n","_, _ = plt.subplots(6, 3, figsize=(12, 24))\n","\n","i = 0\n","for feature in bin_col:\n","    i += 1\n","    plt.subplot(6, 3, i)\n","    \n","    sns.kdeplot(t1[feature], bw=.5, label=\"target = 1\")\n","    sns.kdeplot(t0[feature], bw=.5, label=\"target = 0\")\n","    \n","    plt.ylabel(\"Density plot\", fontsize=12)\n","    plt.xlabel(feature, fontsize=12)\n","    \n","    locs, labels = plt.xticks()\n","    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Checking the correlations between ordinal variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\n","corr_heatmap(v)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature engineering"]},{"cell_type":"markdown","metadata":{},"source":["### creating dummy variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"nominal\") & (meta.keep)].index\n","print(\"Before dummification we have {} variables in train\".format(train.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.get_dummies(train, columns=v, drop_first=True)\n","print(\"After dummification we have {} variables in train\".format(train.shape[1]))"]},{"cell_type":"markdown","metadata":{},"source":["### creating interaction variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["v = meta[(meta.level == \"interval\") & (meta.keep)].index\n","poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n","\n","interactions = pd.DataFrame(data=poly.fit_transform(train[v]),\n","                            columns=poly.get_feature_names(v))\n","interactions.drop(v, axis=1, inplace=True)\n","print(\"Before creating interactions we have {} variables in train\".format(train.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.concat([train, interactions], axis=1)\n","print(\"After creating interactions we have {} variables in train\".format(train.shape[1]))"]},{"cell_type":"markdown","metadata":{},"source":["## Feature selection"]},{"cell_type":"markdown","metadata":{},"source":["### Removing features with low or zero variance"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selector = VarianceThreshold(threshold=.01)\n","selector.fit(train.drop([\"id\", \"target\"], axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["f = np.vectorize(lambda x: not x)\n","v = train.drop([\"id\", \"target\"], axis=1).columns[f(selector.get_support())]\n","print(\"{} variables have too low variance\".format(len(v)),\n","      \"These variables are {}\".format(list(v)), sep=\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["### Feature importance: Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = train.drop([\"id\", \"target\"], axis=1)\n","y_train = train[\"target\"]\n","feat_labels = X_train.columns"]},{"cell_type":"markdown","metadata":{},"source":["rf에 estimator를 1000개 넣더니 제대로 돌아가지 않는다. 주의를 요한다."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# rf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n","rf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4,\n","                            max_features=0.2, n_jobs=-1, random_state=0)\n","rf.fit(X_train, y_train)\n","importances = rf.feature_importances_\n","print(\"Random Forest Training Done!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["indices = np.argsort(rf.feature_importances_)[::-1]\n","for f in range(X_train.shape[1]):\n","    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))"]},{"cell_type":"markdown","metadata":{},"source":["당연히 숫자로만 보기엔 밋밋하니까 시각화!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trace = go.Scatter(\n","    y=importances, x=feat_labels, mode=\"markers\",\n","    marker=dict(sizemode=\"diameter\", sizeref=1, size=13, color=importances,\n","#                 size=importances, color=np.random.randn(500),\n","                colorscale=\"Portland\", showscale=True), text=feat_labels)\n","\n","layout = go.Layout(\n","    autosize=True, title=\"Random Forest Feature Importance\", hovermode=\"closest\",\n","    xaxis=dict(ticklen=5, showgrid=False, zeroline=False, showline=False),\n","    yaxis=dict(title=\"Feature Importance\", ticklen=5, showgrid=False, zeroline=False, gridwidth=2),\n","    showlegend=False)\n","fig = go.Figure(data=[trace], layout=layout)\n","py.iplot(fig, filename=\"scatter2010\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x, y = (list(x) for x in zip(*sorted(zip(importances, feat_labels), reverse=False)))\n","trace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale=\"Viridis\", reversescale=True),\n","               name=\"Random Forest Feature importance\", orientation='h')\n","\n","layout = dict(title=\"Barplot of Feature importances\", width=900, height=2000,\n","              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\n","fig = go.Figure(data=[trace])\n","fig[\"layout\"].update(layout)\n","py.iplot(fig, filename=\"plots\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Selecting features with a Random Forest and SelectFromModel"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sfm = SelectFromModel(rf, threshold=\"median\", prefit=True)\n","print(\"Number of features before selection: {}\".format(X_train.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_features = sfm.transform(X_train).shape[1]\n","print(\"Number of features after selection: {}\".format(n_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_vars = list(feat_labels[sfm.get_support()])\n","train = train[selected_vars + [\"target\"]]"]},{"cell_type":"markdown","metadata":{},"source":["### Feature importance: Gradient Boosting model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4,\n","                                max_features=0.2, random_state=0)\n","gb.fit(X_train, y_train)\n","importances = gb.feature_importances_\n","print(\"Gradient Boosting model Training Done!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trace = go.Scatter(\n","    y=importances, x=feat_labels, mode=\"markers\",\n","    marker=dict(sizemode=\"diameter\", sizeref=1, size=13, color=importances,\n","#                 size=importances, color=np.random.randint(500),\n","                colorscale=\"Portland\", showscale=True), text=feat_labels)\n","\n","layout = go.Layout(\n","    autosize=True, title=\"Gradient Boosting Machine Feature Importance\", hovermode=\"closest\",\n","    xaxis=dict(ticklen=5, showgrid=False, zeroline=False, showline=False),\n","    yaxis=dict(title=\"Feature Importance\", ticklen=5, showgrid=False, zeroline=False,\n","               gridwidth=2), showlegend=False)\n","fig = go.Figure(data=[trace], layout=layout)\n","py.iplot(fig, filename=\"scatter2010\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x, y = (list(x) for x in zip(*sorted(zip(importances, feat_labels), reverse=False)))\n","trace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale=\"Viridis\", reversescale=True),\n","               name=\"Gradient Boosting Classifier Feature importance\", orientation='h')\n","\n","layout = dict(title=\"Barplot of Feature importances\", width=900, height=2000,\n","              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\n","fig = go.Figure(data=[trace])\n","fig[\"layout\"].update(layout)\n","py.iplot(fig, filename=\"plots\")"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dt = tree.DecisionTreeClassifier(max_depth=3)\n","dt.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open(\"tree1.dot\", 'w') as f:\n","#     f = tree.export_graphviz(dt, out_file=f, max_depth=4, impurity=False,\n","#                              feature_name=feat_labels, class_names=[\"No\", \"Yes\"],\n","#                              rounded=True, filled=True)\n","# check_call([\"dot\", \"-Tpng\", \"tree1.dot\", \"-o\", \"tree1.png\"])\n","\n","# img = Image.open(\"tree1.png\")\n","# draw = ImageDraw(img)\n","# img.save(\"sample-out.png\")\n","# PImage(\"sample-out.png\",)"]},{"cell_type":"markdown","metadata":{},"source":["### XGBoost CV\n","\n","tree 결정계수로 많이 이용하는 Gini 계수 값으로 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MAX_ROUNDS = 400\n","OPTIMIZE_ROUNDS = False\n","LEARNING_RATE = 0.07\n","EARLY_STOPPING_ROUNDS = 50"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@jit\n","def eval_gini(y_true, y_prob):\n","    y_true = np.asarray(y_true)\n","    y_true = y_true[np.argsort(y_prob)]\n","    \n","    ntrue = 0, gini = 0, delta = 0\n","    n = len(y_true)\n","    for i in range(n - 1, -1, -1):\n","        yi = y_true[i]\n","        ntrue += yi\n","        gini += yi * delta\n","        delta += 1 - yi\n","    gini = 1 - 2 * gini / (nture * (n - ntrue))\n","    return gini"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def gini_xgb(preds, dtrain):\n","    labels = dtrain.get_label()\n","    gini_score = -eval_gini(labels, preds)\n","    return [(\"gini\", gini_score)]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combs = [(\"ps_reg_01\", \"ps_car_02_cat\"), (\"ps_reg_01\", \"ps_car_04_cat\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["id_train = train.id.values\n","id_test = test.id.values\n","y = train[\"target\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_features = []\n","start = time.time()\n","for nc, (f1, f2) in enumerate(combs):\n","    name = f1 + \"_plus_\" + f2\n","    print(\"current feature %60s %4d in %5.1f\" % (name, nc + 1, (time.time() - start) / 60))\n","    \n","    train[name] = train[f1].apply(lambda x: str(x)) + \"_\" + train[f2].apply(lambda x: str(x))\n","    test[name] = test[f1].apply(lambda x: str(x)) + \"_\" + test[f2].apply(lambda x: str(x))\n","    \n","    lbl = LabelEncoder()\n","    lbl.fit(list(train[name].values) + list(test[name].values))\n","    train[name] = lbl.transform(list(train[name].values))\n","    test[name] = lbl.transform(list(test[name].values))\n","    \n","    new_features.append(name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["f_cats = [meta[(meta.level == \"nominal\") & (meta.keep)].index,\n","          f for f in new_features if \"_cat\" in f]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_valid_pred = 0 * y\n","y_test_pred = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["K = 5\n","kf = KFold(n_splits=K, random_state=1, shuffle=True)\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = XGBClassifier(\n","    n_estimators=MAX_ROUNDS, max_depth=4, objective=\"binary:logistic\",\n","    learning_rate=LEARNING_RATE, subsample=.8, min_child_weight=6, colsample_bytree=.8,\n","    scale_pos_weight=1.6, gamma=10, reg_alpha=8, reg_lambda=1.3)"]},{"cell_type":"markdown","metadata":{},"source":["CV할 거다, 파이팅 컴퓨터."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i, (train_index, test_index) in enumerate(kf.split(train)):\n","    y_train, y_valid = y.iloc[train_index].copy(), y,iloc[test_index]\n","    X_train, X_valid = X.iloc[train_index].copy(), X.iloc[test_index, :].copy()\n","    X_test = test.copy()\n","    print(\"\\nFold\", i)\n","    \n","    for f in f_cats:\n","        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n","            X_train[f], X_test[f], X_valid[f], y_train, 200, 10, 0)\n","    \n","    if OPTIMIZE_ROUNDS:\n","        eval_set = [(X_valid, y_valid)]\n","        fit_model = model.fit(X_train, y_train, eval_set=eval_set, eval_metric=gini_xgb,\n","                              early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n","        print(\"\\tBest N trees =\", model.best_ntree_limit, \"\\tBest gini =\", model.best_score)\n","    else:\n","        fit_model = model.fit(X_train, y_train)\n","    \n","    pred = fit_model.predict(X_valid)[:, 1]\n","    print(\"\\tGini =\", eval_gini(y_valid, pred))\n","    y_valid_pred.iloc[test_index] = pred\n","    y_test_pred += fit_model.predict_proba(X_test)[:, 1]\n","    \n","    del X_test, X_train, X_valid, y_train\n","\n","y_test_pred /= K\n","print(\"\\nGini for full training set:\", eval_gini(y, y_valid_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val = pd.DataFrame()\n","# val[\"id\"] = id_train\n","# val[\"target\"] = y_valid_pred.values\n","# val.to_csv(\"xgb_valid.csv\", float_format=\"%.6f\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sub = pd.DataFrame()\n","# sub[\"id\"] = id_test\n","# sub[\"target\"] = y_test_pred\n","# sub.to_csv(\"xgb_submit.csv\", float_format=\"%.6f\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature scaling\n","\n","scaler = StandardScaler()\n","scaler.fit_transform(train.drop([\"target\"], axis=1))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
