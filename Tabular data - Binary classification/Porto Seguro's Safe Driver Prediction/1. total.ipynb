{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Binary classification\n\n- Porto Seguro's Safe Driver Prediction\n- [자료1](https://www.kaggle.com/bertcarremans/data-preparation-exploration), [자료2](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial), [자료3](https://www.kaggle.com/aharless/xgboost-cv-lb-284), [자료4](https://www.kaggle.com/gpreda/porto-seguro-exploratory-analysis-and-prediction)","metadata":{}},{"cell_type":"markdown","source":"## Loading","metadata":{}},{"cell_type":"code","source":"# 기본: 데이터 다루기\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw, ImageFont\n# 정규식\nimport re\nimport time\n\n# Numba: python 연산을 더 빠르게 해주는 compiler를 이용할 수 있게 하는 library\nfrom numba import jit\n# compile 후 이용하듯 먼저 작은 값으로 연산시켜서 구성을 저장하고 실제 큰 값을 최적화시켜 이용하는 방식\nimport gc # Garbage Collector\n\nfrom collections import Counter\nimport missingno as msno\n\n# Sklearn package, 모델 적합에 이용\n# sklearn.preprocessing.Imputer는 0.22 ver에서 삭제되었다.\nfrom sklearn.utils import shuffle\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder\n\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel, mutual_info_classif\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom IPython.display import Image as PImage\n\n# 새로운 plotting package 등장\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\n# option\npd.set_option(\"display.max_columns\", 100)\nfrom subprocess import check_call\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/porto-seguros-safe-driver-prediction-dataset/train.csv\")\ntest = pd.read_csv(\"../input/porto-seguros-safe-driver-prediction-dataset/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"내 이럴 줄 알았다. inplace 매개변수를 쓰던, output을 원 데이터로 해주던 초기화해주는 장치가 있어야 drop_duplicates()한 것을 저장할 수 있다.","metadata":{}},{"cell_type":"markdown","source":"## preprocessing: Metadata\n\n- feature를 이용한 변수 분석, 시각화, 모델링 등에 도움이 되어 (좀 더 실정에 맞는 모델을 구성할 수 있음) 데이터 DataFrame 자체의 metadata를 아는 것은 중요하다.\n- 그런데 여기서 얘기하는 metadata는 feature 데이터의 자체적 특성(or 분류)를 말하는 것 같다.","metadata":{}},{"cell_type":"markdown","source":"- **role**: input, ID, target\n- **level**: nominal, interval, ordinal, binary\n- **keep**: True or False\n- **dtype**: int, float, str","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    if f == \"target\":\n        role = \"target\"\n    elif f == \"id\":\n        role = \"id\"\n    else:\n        role = \"input\"\n    \n    if \"bin\" in f or f == \"target\":\n        level = \"binary\"\n    elif \"cat\" in f or f == \"id\":\n        level = \"nominal\"\n    elif train[f].dtype == float:\n        level = \"interval\"\n    else:\n        level = \"ordinal\"\n    \n    keep = True\n    if f == \"id\":\n        keep = False\n    \n    category = \"none\"\n    if \"ind\" in f:\n        category = \"individual\"\n    elif \"reg\" in f:\n        category = \"registration\"\n    elif \"car\" in f:\n        category = \"car\"\n    elif \"calc\" in f:\n        category = \"calculated\"\n    \n    dtype = train[f].dtype\n    f_dict = {\"varname\": f, \"role\": role, \"level\": level, \"keep\": keep, \"dtype\": dtype,\n              \"category\": category}\n    data.append(f_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta = pd.DataFrame(data, columns=[\"varname\", \"role\", \"level\", \"keep\", \"dtype\", \"category\"])\nmeta.set_index(\"varname\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta[(meta.level == \"nominal\") & (meta.keep)].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"count\": meta.groupby([\"category\"])[\"category\"].size()}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"count\": meta.groupby([\"role\", \"level\"])[\"role\"].size()}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive statistics\n\n- explore the categorical variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"reg variables\n\n- -1 = NA이기 때문에 reg_03에서 결측값 존재 (설명에선 하나래)\n- 다른 reg에 비해 (reg_03의) max 값이 꽤 큰 편이라 표준화시킬 예정인가봄\n\ncar variables\n\n- car_12, car_14에 NA 존재\n- 여기도 13, 15의 max가 큰 편이라 표준화시킬 생각인 듯하다.\n\ncalc variables\n\n- not missing values\n- 굳이 표준화시킬 필요 없음.","metadata":{}},{"cell_type":"markdown","source":"### Ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"car_11에만 missing values","metadata":{}},{"cell_type":"markdown","source":"### Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"binary\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"not missing values, and don't use scaler","metadata":{}},{"cell_type":"markdown","source":"## Handling imbalanced classes","metadata":{}},{"cell_type":"markdown","source":"#### 작업하기 전에 target data 분포에 대해 확인 먼저 하기","metadata":{}},{"cell_type":"code","source":"data = [go.Bar(x=train[\"target\"].value_counts().index.values,\n               y=train[\"target\"].value_counts().values,\n               text=\"Distribution of target variable\")]\n\nlayout = go.Layout(title=\"Target variable distribution\")\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"basic-bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이를 통해 target data가 불균형하게 분포하고 있음을 알 수 있다. 불균형을 최소화하는 방향으로.","metadata":{}},{"cell_type":"code","source":"Counter(train.dtypes.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desired_apriori = 0.10\n\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\nundersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\nundersampled_nb_0 = int(undersampling_rate * nb_0)\nprint(\"Rate to undersample records with target=0: {}\".format(undersampling_rate),\n      \"Number of records with target=0 after undersampling: {}\".format(undersampled_nb_0),\n      sep=\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\nidx_list = list(undersampled_idx) + list(idx_1)\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Quality Checks\n\n결측값 찾기","metadata":{}},{"cell_type":"code","source":"train.isnull().any().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings / train.shape[0]\n        \n        print(\"Variables {} has {} records ({:.2%}) with missing values\"\n             .format(f, missings, missings_perc))\n\nprint(\"In total, there are {} variables with missing values\".format(len(vars_with_missing)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(train_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"생각보다 결측값이 많네. car_02랑 car_11만 없는 것으로 보인다.","metadata":{}},{"cell_type":"code","source":"vars_to_drop = [\"ps_car_03_cat\", \"ps_car_05_cat\"]\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop), \"keep\"] = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_imp = SimpleImputer(missing_values=-1, strategy=\"mean\")\nmode_imp = SimpleImputer(missing_values=-1, strategy=\"mose_frequent\")\ntrain[\"ps_reg_03\"] = mean_imp.fit_transform(train[[\"ps_reg_03\"]]).ravel()\ntrain[\"ps_car_12\"] = mean_imp.fit_transform(train[[\"ps_car_12\"]]).ravel()\ntrain[\"ps_car_14\"] = mean_imp.fit_transform(train[[\"ps_car_14\"]]).ravel()\ntrain[\"ps_car_11\"] = mean_imp.fit_transform(train[[\"ps_car_11\"]]).ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the cardinality of the categorical variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print(\"Variable {} has {} distinct values\".format(f, dist_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Script by https://www.kaggle.com/ogrellier<br />\nCode: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"assert: 가정 설정문\n\n- 형식: assert 조건, 메시지\n- 조건을 만족하지 않으면 AssertError를 준다.","metadata":{}},{"cell_type":"code","source":"def target_encode(trn_series=None, tst_series=None, val_series=None, target=None,\n                  min_samples_leaf=1, smoothing=1, noise_level=0):\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    \n    temp = pd.concat([trn_series, target], axis=1)\n    averages = temp.groupby(trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    prior = target.mean()\n    \n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    \n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n        on=trn_series.name, how=\"left\"\n    )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n    ft_trn_series.index = trn_series.index\n    \n    \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n        on=tst_series.name, how=\"left\"\n    )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n    ft_tst_series.index = tst_series.index\n    \n    if val_series is not None:\n        ft_val_series = pd.merge(\n            val_series.to_frame(val_series.name),\n            averages.reset_index().rename(columns={\"index\": target.name, target.name: \"average\"}),\n            on=val_series.name, how=\"left\"\n        )[\"average\"].rename(trn_series.name + \"_mean\").fillna(prior)\n        ft_val_series.index = val_series.index\n        \n        return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], test[\"ps_car_11_cat\"],\n                                            target=train.target, min_samples_leaf=100,\n                                            smoothing=10, noise_level=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"ps_car_11_cat_te\"] = train_encoded\ntrain.drop(\"ps_car_11_cat\", axis=1, inplace=True)\n\nmeta.loc[\"ps_car_11_cat\", \"keep\"] = False\n\ntest[\"ps_car_11_cat_te\"] = test_encoded\ntest.drop(\"ps_car_11_cat\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Visualization","metadata":{}},{"cell_type":"markdown","source":"### Categorical variables\n\n- \\_, ax = plt.subplots(3, 4)로 구제하고 싶었지만, sns라 그럴 수 없었다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\nfor f in v:\n    plt.figure(figsize=(20, 10))\n    \n    cat_perc = train[[f, \"target\"]].groupby([f], as_index=False).mean()\n    cat_perc.sort_values(\"target\", ascending=False, inplace=True)\n    \n    sns.barplot(f, \"target\", data=cat_perc, order=cat_perc[f])\n    \n    plt.ylabel(\"% target\", fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"데이터 타입만 고려한 거라 다를 수 있지만, interval이 float를 대변하기에 int에 관한 것도 찍어봤다.","metadata":{}},{"cell_type":"code","source":"# ordinal이 int가 맞을까\nv = meta[(meta.dtype == int) & (meta.keep)].index\nplotting_data = [go.Heatmap(\n    z=train[v].corr().values, x=train[v].columns.values, y=train[v].columns.values,\n    colorscale=\"Viridis\", reversescale=False, opacity=1.0)] # text=True\n\nlayout = go.Layout(\n    title=\"Pearson Correlation of Integer-type features\",\n    xaxis=dict(ticks='', nticks=36), yaxis=dict(ticks=''), width=900, height=700)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"labelled-heatmap\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interval variables","metadata":{}},{"cell_type":"code","source":"intervals = meta[(meta.level == \"interval\") & (meta.keep)].index\ntargets = meta[(meta.role == \"target\")].index\n\nmf = mutual_info_classif(train[intervals].values, train[targets].values, n_neighbors=3,\n                         random_state=17)\nprint(mf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN distance(entropy)를 기반으로 interval type input과 target 간의 밀접도 또는 의존도(결과적으로 상관계수)를 확인했다.","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n#     cmap = plt.cm.magma\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt=\".2f\",\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"고려해야 할 상관계수\n\n- reg_02, reg_03: 0.7\n- car_12, car_13: 0.67\n- car_12, car_14: 0.58\n- car_13, car_15: 0.53","metadata":{}},{"cell_type":"markdown","source":"컴퓨터 부담 덜 주겠다고 sampling하시겠답디다. 그리고 sampling했기 때문에 그림이 다를 수 있다.","metadata":{}},{"cell_type":"code","source":"s = train.sample(frac=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15, 15))\nsns.lmplot(\"ps_reg_02\", \"ps_reg_03\", data=s, hue=\"target\", palette=\"Set1\",\n           scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15, 15))\nsns.lmplot(\"ps_car_12\", \"ps_car_13\", data=s, hue=\"target\", palette=\"Set1\",\n           scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15, 15))\nsns.lmplot(\"ps_car_12\", \"ps_car_14\", data=s, hue=\"target\", palette=\"Set1\",\n           scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15, 15))\nsns.lmplot(\"ps_car_13\", \"ps_car_15\", data=s, hue=\"target\", palette=\"Set1\",\n           scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"심심하니까 ps_car_12랑 ps_car_15도 확인해보자.","metadata":{}},{"cell_type":"code","source":"sample = s[[\"ps_car_12\", \"ps_car_15\", \"target\"]]\nsns.pairplot(sample, hue=\"target\", palette=\"Set1\", diag_kind=\"kde\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"저것만 보기엔 그래프가 굉장히 많으 정보를 담고 있으니 상관계수가 높은 feature는 다 봐보자.","metadata":{}},{"cell_type":"code","source":"var = [\"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\", \"ps_car_15\", \"target\"]\nsample = s[var]\nsns.pairplot(sample, hue=\"target\", palette=\"Set1\", diag_kind=\"kde\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"target에 대해 가우스 분포 쪽으로 한 번 더 확인해보자.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\nt1, t0 = train.loc[train[\"target\"] != 0], train.loc[train[\"target\"] == 0]\n\n# sns.set_style(\"whitegrid\")\nplt.figure()\n_, _ = plt.subplots(3, 4, figsize=(16, 12))\n\ni = 0\nfor feature in v:\n    i += 1\n    plt.subplot(3, 4, i)\n    \n    sns.kdeplot(t1[feature], bw=.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=.5, label=\"target = 0\")\n    \n    plt.ylabel(\"Density plot\", fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the Binary features inspection\n\n0, 1 비율을 모든 binary에 대해 한 번에 나타내고자 함.","metadata":{}},{"cell_type":"code","source":"bin_col = meta[(meta.level == \"binary\") & (meta.keep)].index\nbin_col = train[bin_col].columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col] == 0).sum())\n    one_list.append((train[col] == 1).sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Bar(x=bin_col, y=zero_list, name=\"Zero count\")\ntrace2 = go.Bar(x=bin_col, y=one_list, name=\"One count\")\n\nlayout = go.Layout(barmode=\"stack\", title=\"Count of 1 and 0 in binary variables\")\nfig = go.Figure(data=[trace1, trace2], layout=layout)\npy.iplot(fig, filename=\"stacked-bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"interval이 그랬던 것처럼 얘도 가우스분포(앞에도 그랬지만, 다차항이 아니고 정확히 정규분포도 아니라서 그냥 종모양 분포를 얘기하는 것)로 확인해보자.","metadata":{}},{"cell_type":"code","source":"# sns.set_style(\"whitegrid\")\nplt.figure()\n_, _ = plt.subplots(6, 3, figsize=(12, 24))\n\ni = 0\nfor feature in bin_col:\n    i += 1\n    plt.subplot(6, 3, i)\n    \n    sns.kdeplot(t1[feature], bw=.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=.5, label=\"target = 0\")\n    \n    plt.ylabel(\"Density plot\", fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    \n    locs, labels = plt.xticks()\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the correlations between ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"markdown","source":"### creating dummy variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\nprint(\"Before dummification we have {} variables in train\".format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.get_dummies(train, columns=v, drop_first=True)\nprint(\"After dummification we have {} variables in train\".format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]),\n                            columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)\nprint(\"Before creating interactions we have {} variables in train\".format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, interactions], axis=1)\nprint(\"After creating interactions we have {} variables in train\".format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection","metadata":{}},{"cell_type":"markdown","source":"### Removing features with low or zero variance","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop([\"id\", \"target\"], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = np.vectorize(lambda x: not x)\nv = train.drop([\"id\", \"target\"], axis=1).columns[f(selector.get_support())]\nprint(\"{} variables have too low variance\".format(len(v)),\n      \"These variables are {}\".format(list(v)), sep=\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance: Random Forest","metadata":{}},{"cell_type":"code","source":"X_train = train.drop([\"id\", \"target\"], axis=1)\ny_train = train[\"target\"]\nfeat_labels = X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"rf에 estimator를 1000개 넣더니 제대로 돌아가지 않는다. 주의를 요한다.","metadata":{}},{"cell_type":"code","source":"# rf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4,\n                            max_features=0.2, n_jobs=-1, random_state=0)\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\nprint(\"Random Forest Training Done!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = np.argsort(rf.feature_importances_)[::-1]\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"당연히 숫자로만 보기엔 밋밋하니까 시각화!","metadata":{}},{"cell_type":"code","source":"trace = go.Scatter(\n    y=importances, x=feat_labels, mode=\"markers\",\n    marker=dict(sizemode=\"diameter\", sizeref=1, size=13, color=importances,\n#                 size=importances, color=np.random.randn(500),\n                colorscale=\"Portland\", showscale=True), text=feat_labels)\n\nlayout = go.Layout(\n    autosize=True, title=\"Random Forest Feature Importance\", hovermode=\"closest\",\n    xaxis=dict(ticklen=5, showgrid=False, zeroline=False, showline=False),\n    yaxis=dict(title=\"Feature Importance\", ticklen=5, showgrid=False, zeroline=False, gridwidth=2),\n    showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\npy.iplot(fig, filename=\"scatter2010\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(importances, feat_labels), reverse=False)))\ntrace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale=\"Viridis\", reversescale=True),\n               name=\"Random Forest Feature importance\", orientation='h')\n\nlayout = dict(title=\"Barplot of Feature importances\", width=900, height=2000,\n              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\nfig = go.Figure(data=[trace])\nfig[\"layout\"].update(layout)\npy.iplot(fig, filename=\"plots\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Selecting features with a Random Forest and SelectFromModel","metadata":{}},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold=\"median\", prefit=True)\nprint(\"Number of features before selection: {}\".format(X_train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = sfm.transform(X_train).shape[1]\nprint(\"Number of features after selection: {}\".format(n_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_vars = list(feat_labels[sfm.get_support()])\ntrain = train[selected_vars + [\"target\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance: Gradient Boosting model","metadata":{}},{"cell_type":"code","source":"gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4,\n                                max_features=0.2, random_state=0)\ngb.fit(X_train, y_train)\nimportances = gb.feature_importances_\nprint(\"Gradient Boosting model Training Done!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Scatter(\n    y=importances, x=feat_labels, mode=\"markers\",\n    marker=dict(sizemode=\"diameter\", sizeref=1, size=13, color=importances,\n#                 size=importances, color=np.random.randint(500),\n                colorscale=\"Portland\", showscale=True), text=feat_labels)\n\nlayout = go.Layout(\n    autosize=True, title=\"Gradient Boosting Machine Feature Importance\", hovermode=\"closest\",\n    xaxis=dict(ticklen=5, showgrid=False, zeroline=False, showline=False),\n    yaxis=dict(title=\"Feature Importance\", ticklen=5, showgrid=False, zeroline=False,\n               gridwidth=2), showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\npy.iplot(fig, filename=\"scatter2010\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(importances, feat_labels), reverse=False)))\ntrace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale=\"Viridis\", reversescale=True),\n               name=\"Gradient Boosting Classifier Feature importance\", orientation='h')\n\nlayout = dict(title=\"Barplot of Feature importances\", width=900, height=2000,\n              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\nfig = go.Figure(data=[trace])\nfig[\"layout\"].update(layout)\npy.iplot(fig, filename=\"plots\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree visualization","metadata":{}},{"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(\"tree1.dot\", 'w') as f:\n#     f = tree.export_graphviz(dt, out_file=f, max_depth=4, impurity=False,\n#                              feature_name=feat_labels, class_names=[\"No\", \"Yes\"],\n#                              rounded=True, filled=True)\n# check_call([\"dot\", \"-Tpng\", \"tree1.dot\", \"-o\", \"tree1.png\"])\n\n# img = Image.open(\"tree1.png\")\n# draw = ImageDraw(img)\n# img.save(\"sample-out.png\")\n# PImage(\"sample-out.png\",)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost CV\n\ntree 결정계수로 많이 이용하는 Gini 계수 값으로 보자.","metadata":{}},{"cell_type":"code","source":"MAX_ROUNDS = 400\nOPTIMIZE_ROUNDS = False\nLEARNING_RATE = 0.07\nEARLY_STOPPING_ROUNDS = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    \n    ntrue = 0, gini = 0, delta = 0\n    n = len(y_true)\n    for i in range(n - 1, -1, -1):\n        yi = y_true[i]\n        ntrue += yi\n        gini += yi * delta\n        delta += 1 - yi\n    gini = 1 - 2 * gini / (nture * (n - ntrue))\n    return gini","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = -eval_gini(labels, preds)\n    return [(\"gini\", gini_score)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combs = [(\"ps_reg_01\", \"ps_car_02_cat\"), (\"ps_reg_01\", \"ps_car_04_cat\")]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_train = train.id.values\nid_test = test.id.values\ny = train[\"target\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_features = []\nstart = time.time()\nfor nc, (f1, f2) in enumerate(combs):\n    name = f1 + \"_plus_\" + f2\n    print(\"current feature %60s %4d in %5.1f\" % (name, nc + 1, (time.time() - start) / 60))\n    \n    train[name] = train[f1].apply(lambda x: str(x)) + \"_\" + train[f2].apply(lambda x: str(x))\n    test[name] = test[f1].apply(lambda x: str(x)) + \"_\" + test[f2].apply(lambda x: str(x))\n    \n    lbl = LabelEncoder()\n    lbl.fit(list(train[name].values) + list(test[name].values))\n    train[name] = lbl.transform(list(train[name].values))\n    test[name] = lbl.transform(list(test[name].values))\n    \n    new_features.append(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_cats = [meta[(meta.level == \"nominal\") & (meta.keep)].index,\n          f for f in new_features if \"_cat\" in f]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_pred = 0 * y\ny_test_pred = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBClassifier(\n    n_estimators=MAX_ROUNDS, max_depth=4, objective=\"binary:logistic\",\n    learning_rate=LEARNING_RATE, subsample=.8, min_child_weight=6, colsample_bytree=.8,\n    scale_pos_weight=1.6, gamma=10, reg_alpha=8, reg_lambda=1.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CV할 거다, 파이팅 컴퓨터.","metadata":{}},{"cell_type":"code","source":"for i, (train_index, test_index) in enumerate(kf.split(train)):\n    y_train, y_valid = y.iloc[train_index].copy(), y,iloc[test_index]\n    X_train, X_valid = X.iloc[train_index].copy(), X.iloc[test_index, :].copy()\n    X_test = test.copy()\n    print(\"\\nFold\", i)\n    \n    for f in f_cats:\n        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n            X_train[f], X_test[f], X_valid[f], y_train, 200, 10, 0)\n    \n    if OPTIMIZE_ROUNDS:\n        eval_set = [(X_valid, y_valid)]\n        fit_model = model.fit(X_train, y_train, eval_set=eval_set, eval_metric=gini_xgb,\n                              early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n        print(\"\\tBest N trees =\", model.best_ntree_limit, \"\\tBest gini =\", model.best_score)\n    else:\n        fit_model = model.fit(X_train, y_train)\n    \n    pred = fit_model.predict(X_valid)[:, 1]\n    print(\"\\tGini =\", eval_gini(y_valid, pred))\n    y_valid_pred.iloc[test_index] = pred\n    y_test_pred += fit_model.predict_proba(X_test)[:, 1]\n    \n    del X_test, X_train, X_valid, y_train\n\ny_test_pred /= K\nprint(\"\\nGini for full training set:\", eval_gini(y, y_valid_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val = pd.DataFrame()\n# val[\"id\"] = id_train\n# val[\"target\"] = y_valid_pred.values\n# val.to_csv(\"xgb_valid.csv\", float_format=\"%.6f\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub = pd.DataFrame()\n# sub[\"id\"] = id_test\n# sub[\"target\"] = y_test_pred\n# sub.to_csv(\"xgb_submit.csv\", float_format=\"%.6f\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature scaling\n\nscaler = StandardScaler()\nscaler.fit_transform(train.drop([\"target\"], axis=1))","metadata":{}}]}